{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "is_front_camera_training = False  # Change it to False if training for down cmaera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# OS PARAMETERS ##################################\n",
    "os_name = os.name\n",
    "# Windows = nt, [Linux, Apple] = posix. \n",
    "os_path = \"\\\\\" if os_name == \"nt\" else \"/\"  \n",
    "##################################################################################\n",
    "############################## ROBOFLOW  PARAMETERS ##############################\n",
    "roboflow_api_key = \"MJQUATZvpcKoBxRjLuXx\"\n",
    "roboflow_workspace_name = \"auv2024\"\n",
    "dataset_export_format = \"yolov8\"\n",
    "if is_front_camera_training:\n",
    "     roboflow_project_name = \"down-camera-comp\"\n",
    "else:\n",
    "     roboflow_project_name = \"down-camera-sim\"\n",
    "##################################################################################\n",
    "############################## TRAINING  PARAMETERS ##############################\n",
    "if is_front_camera_training:\n",
    "     target_classes = [\"bouy\", \"gate\", \"octagon-table\"]\n",
    "     model_save_filename = \"best_AUV_sim_front_camera_model.pt\"\n",
    "else: \n",
    "     target_classes = [\"bin\", \"lane-marker\", \"octagon-table\"]\n",
    "     model_save_filename = \"best_AUV_sim_down_camera_model.pt\"\n",
    "model_name = \"yolov8n.pt\"\n",
    "train_test_val_split = (0.7, 0.2, 0.1)\n",
    "epoch_increments = 200\n",
    "batch_size = -1 # Auto Mode (60% GPU Memory): Use batch=-1 to automatically \n",
    "                # adjust batch size for approximately 60% CUDA memory \n",
    "                # utilization.\n",
    "workers = 2\n",
    "cache = False\n",
    "pretrained = True\n",
    "imgsz = [480, 640]\n",
    "hsv_h = 0.015\n",
    "hsv_s = 0.3\n",
    "hsv_v = 0.3\n",
    "translate = 0.0\n",
    "scale = 0.0\n",
    "fliplr = 0.5\n",
    "flipud = 0.5\n",
    "mosaic = 0.1\n",
    "copy_paste = 0.0\n",
    "erasing = 0.0\n",
    "crop_fraction = 0.1\n",
    "degrees = 180\n",
    "##################################################################################\n",
    "######################### CUSTOM AUGMENTATION PARAMETERS #########################\n",
    "colorAugmentProb = 0.5\n",
    "noiseAugmentProb = 0.5\n",
    "resolutionAugmentProb = 0.0\n",
    "contrastAugmentProb = 0.5\n",
    "blurAugmentProb = 0.5\n",
    "brightnessAugmentProb = 0.5\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup python dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (1.2.9)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2025.8.3)\n",
      "Requirement already satisfied: idna==3.7 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.4.9)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (3.10.6)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2.2.6)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (11.3.0)\n",
      "Requirement already satisfied: pi-heif<2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.1.0)\n",
      "Requirement already satisfied: pillow-avif-plugin<2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.1.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2.32.5)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib->roboflow) (1.3.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib->roboflow) (4.60.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib->roboflow) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib->roboflow) (3.2.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests->roboflow) (3.3.2)\n",
      "Requirement already satisfied: albumentations in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (1.16.2)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (2.11.9)\n",
      "Requirement already satisfied: albucore==0.0.24 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albucore==0.0.24->albumentations) (4.0.13)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from opencv-python) (2.2.6)\n",
      "Requirement already satisfied: ultralytics in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (8.3.202)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (3.10.6)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (1.16.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (2.8.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (0.23.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: polars in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (1.33.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (2.0.17)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.2-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n",
      "Packages installed successfully.\n"
     ]
    }
   ],
   "source": [
    "packages = [\n",
    "     \"roboflow\", \n",
    "     \"albumentations\", \n",
    "     \"opencv-python\", \n",
    "     \"ultralytics\",\n",
    "     \"pandas\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    print(\"Packages installed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"An error occurred: {e}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from roboflow import Roboflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "mkdir: data/augmented: File exists\n",
      "mkdir: data/augmented/train: File exists\n",
      "mkdir: data/augmented/test: File exists\n",
      "mkdir: data/augmented/val: File exists\n",
      "mkdir: data/augmented/train/images: File exists\n",
      "mkdir: data/augmented/test/images: File exists\n",
      "mkdir: data/augmented/val/images: File exists\n",
      "mkdir: data/augmented/train/labels: File exists\n",
      "mkdir: data/augmented/test/labels: File exists\n",
      "mkdir: data/augmented/val/labels: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!mkdir data/augmented\n",
    "!mkdir data/augmented/images\n",
    "!mkdir data/augmented/labels\n",
    "!mkdir data/augmented/train\n",
    "!mkdir data/augmented/test\n",
    "!mkdir data/augmented/val\n",
    "!mkdir data/augmented/train/images\n",
    "!mkdir data/augmented/test/images\n",
    "!mkdir data/augmented/val/images\n",
    "!mkdir data/augmented/train/labels\n",
    "!mkdir data/augmented/test/labels\n",
    "!mkdir data/augmented/val/labels\n",
    "!mkdir data/raw\n",
    "!mkdir data/raw/images\n",
    "!mkdir data/raw/labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define YOLO classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data absolute path: /Users/owenlesann/robotics/AUV-2025/catkin_ws/src/vision/model_pipeline/data.\n"
     ]
    }
   ],
   "source": [
    "data_folder_absolute_path = os.path.abspath(\"data\")\n",
    "print(f\"Data absolute path: {data_folder_absolute_path}.\")\n",
    "\n",
    "with open(\"data.yaml\", \"w+\") as f:\n",
    "    f.write(f\"train: {data_folder_absolute_path}{os_path}augmented{os_path}train{os_path}images\\n\")\n",
    "    f.write(f\"test: {data_folder_absolute_path}{os_path}augmented{os_path}test{os_path}images\\n\")\n",
    "    f.write(f\"val: {data_folder_absolute_path}{os_path}augmented{os_path}val{os_path}images\\n\")\n",
    "    f.write(f\"nc: {len(target_classes)}\\n\")\n",
    "    f.write(f\"names: {target_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of samples, make two copies of each sample that are darker/brighter to simulate differently lit environments.\n",
    "def brightnessAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([A.ColorJitter(brightness=(1.5, 1.5), contrast=0, saturation=0, hue=0, always_apply=True)])\n",
    "        bright_img = transform(image=image)[\"image\"]\n",
    "        transform = A.Compose([A.ColorJitter(brightness=(0.5, 0.5), contrast=0, saturation=0, hue=0, always_apply=True)])\n",
    "        dark_img = transform(image=image)[\"image\"]\n",
    "        out.append(bright_img)\n",
    "        out.append(dark_img)\n",
    "    return out\n",
    "\n",
    "# Given a list of samples, make a copy of each sample but more blurred to simulate objects out of focus, dirty lenses, and backscattering.\n",
    "def blurAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        ksize = (8, 8) # lower to lower blur\n",
    "        blurred_img = cv2.blur(image, ksize)\n",
    "        out.append(blurred_img)\n",
    "    return out\n",
    "\n",
    "# Given a list of samples, make a copy of each sample but with a lower contrast image to simulate backscattering and over/under-exposure.\n",
    "def contrastAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([A.ColorJitter (brightness=0, contrast=(0.5, 0.5), saturation=0, hue=0, always_apply=True)])\n",
    "        decontrasted_img = transform(image=image)[\"image\"]\n",
    "        out.append(decontrasted_img)\n",
    "    return out\n",
    "\n",
    "# Given a list of samples, make a copy of each sample but with camera noise added to the image to simulate different camera feeds.\n",
    "def noiseAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([A.ISONoise(color_shift=(0.1, 0.1), intensity=(0.5, 0.5), always_apply=True)])\n",
    "        noisy_img = transform(image=image)[\"image\"]\n",
    "        out.append(noisy_img)\n",
    "    return out\n",
    "\n",
    "# Given a list of samples, make a copy of each sample but with the image downscaled (lower resolution of image) to simulate lower quality cameras/images.\n",
    "def resolutionAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([A.Downscale(scale_min=0.2, scale_max=0.2, always_apply=True)])\n",
    "        low_res_img = transform(image=image)[\"image\"]\n",
    "        out.append(low_res_img)\n",
    "    return out\n",
    "\n",
    "# Increase intensity of blues in given image.\n",
    "def make_bluer(img, color_shift_intensity):\n",
    "    img_b, img_g, img_r = cv2.split(img) # Split by channel.\n",
    "    img_b = np.uint16(img_b)\n",
    "    img_b += color_shift_intensity\n",
    "    np.clip(img_b, 0, 255, out=img_b)\n",
    "    img_b = np.uint8(img_b)\n",
    "    img = cv2.merge((img_b, img_g, img_r)) # Merge adjusted channels.\n",
    "    del img_b\n",
    "    del img_g\n",
    "    del img_r\n",
    "    return img\n",
    "\n",
    "# Increase intensity of greens in given image.\n",
    "def make_greener(img, color_shift_intensity):\n",
    "    img_b, img_g, img_r = cv2.split(img) # Split by channel.\n",
    "    img_g = np.uint16(img_g)\n",
    "    img_g += color_shift_intensity\n",
    "    np.clip(img_g, 0, 255, out=img_g)\n",
    "    img_g = np.uint8(img_g)\n",
    "    img = cv2.merge((img_b, img_g, img_r)) # Merge adjusted channels.\n",
    "    del img_b\n",
    "    del img_g\n",
    "    del img_r\n",
    "    return img\n",
    "\n",
    "# Given a list of samples, make two copies of each sample (one bluer, one greener) to simulate different pools + color attenuation.\n",
    "def colorAugment(images):\n",
    "    out = []\n",
    "    color_shift_intensity = int(255*0.05)\n",
    "    for image in images:\n",
    "        blue_img = make_bluer(image, color_shift_intensity)\n",
    "        green_img = make_greener(image, color_shift_intensity)\n",
    "        out.append(blue_img)\n",
    "        out.append(green_img)\n",
    "    return out\n",
    "\n",
    "# Given a single image and augmentation function, displays the image before and images after augmentation.\n",
    "def visualizeAugmentation(img, aug):\n",
    "    # Show original image.\n",
    "    cv2.imshow(\"og\", img)\n",
    "    cv2.waitKey(0)\n",
    "    # Show all augmented images.\n",
    "    for augmented in aug([(img, \"\")])[1:]:\n",
    "        cv2.imshow(\"augmented\", augmented[0])\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(source_folder):\n",
    "    label_filenames = []\n",
    "    img_filenames = [f for f in os.listdir(source_folder + f\"{os_path}images\") if os.path.isfile(os.path.join(source_folder + f\"{os_path}images\", f))]\n",
    "    for img_filename in img_filenames:\n",
    "        label_filenames.append(os.path.splitext(img_filename)[0] + \".txt\")\n",
    "\n",
    "    return np.array(img_filenames), np.array(label_filenames)\n",
    "\n",
    "def split_and_move_data(source_dir, dest_dir, split_ratio=(0.7, 0.2, 0.1)):\n",
    "    source_image_dir = f\"{source_dir}{os_path}images\"\n",
    "    source_label_dir = f\"{source_dir}{os_path}labels\"\n",
    "    # Get list of image and label files.\n",
    "    image_files = sorted(os.listdir(source_image_dir))\n",
    "    label_files = sorted(os.listdir(source_label_dir))\n",
    "\n",
    "    # Shuffle the indices.\n",
    "    indices = np.arange(len(image_files))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split indices.\n",
    "    total_count = len(image_files)\n",
    "    train_end = int(total_count * split_ratio[0])\n",
    "    val_end = train_end + int(total_count * split_ratio[1])\n",
    "\n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "\n",
    "    # Function to move files\n",
    "    def move_files(indices, split):\n",
    "        for i in indices:\n",
    "            shutil.move(os.path.join(source_image_dir, image_files[i]), \n",
    "                        os.path.join(dest_dir, split, 'images', image_files[i]))\n",
    "            shutil.move(os.path.join(source_label_dir, label_files[i]), \n",
    "                        os.path.join(dest_dir, split, 'labels', label_files[i]))\n",
    "\n",
    "    # Move files to corresponding folders\n",
    "    move_files(train_indices, 'train')\n",
    "    move_files(val_indices, 'val')\n",
    "    move_files(test_indices, 'test')\n",
    "\n",
    "def get_augs(img_filename, source_folder):\n",
    "    img = cv2.imread(source_folder + f\"{os_path}images{os_path}\" + img_filename)\n",
    "    augs = [img]\n",
    "    if(np.random.rand() < colorAugmentProb):\n",
    "        augs = augs + colorAugment(augs)\n",
    "    if(np.random.rand() < noiseAugmentProb):\n",
    "        augs = augs + noiseAugment(augs)\n",
    "    if(np.random.rand() < resolutionAugmentProb):\n",
    "        augs = augs + resolutionAugment(augs)\n",
    "    if(np.random.rand() < contrastAugmentProb):\n",
    "        augs = augs + contrastAugment(augs)\n",
    "    if(np.random.rand() < blurAugmentProb):\n",
    "        augs = augs + blurAugment(augs)\n",
    "    if(np.random.rand() < brightnessAugmentProb):\n",
    "        augs = augs + brightnessAugment(augs)\n",
    "    return augs\n",
    "\n",
    "def do_augs_and_export(img_filenames, label_filenames, source_folder, output_folder):\n",
    "    name_num = 1\n",
    "    for (img_filename, label_filename) in zip(img_filenames, label_filenames):\n",
    "        augs = get_augs(img_filename, source_folder)\n",
    "        with open(source_folder + f\"{os_path}labels{os_path}\" + label_filename) as f:\n",
    "            #build array of bounding boxes (each line its own element)\n",
    "            bounding_boxes = f.read()\n",
    "        for aug in augs:\n",
    "            cv2.imwrite(output_folder + f\"{os_path}images{os_path}img\" + str(name_num) + \".png\", aug)\n",
    "            with open(output_folder + f\"{os_path}labels{os_path}img\" + str(name_num) + \".txt\", \"w+\") as f:\n",
    "                f.write(bounding_boxes)\n",
    "            name_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in down-camera-sim-8 to yolov8:: 100%|█| 6980/69"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to down-camera-sim-8 in yolov8:: 100%|█| 840/840 \n"
     ]
    }
   ],
   "source": [
    "rf = Roboflow(api_key=roboflow_api_key)\n",
    "project = rf.workspace(roboflow_workspace_name).project(roboflow_project_name)\n",
    "latest_version = int(project.versions()[0].version)\n",
    "version = project.version(latest_version)\n",
    "dataset = version.download(dataset_export_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roboflow folder name: down-camera-sim-8.\n",
      "Moving train folder...\n",
      "Moving train folder completed.\n",
      "Moving valid folder...\n",
      "Moving valid folder completed.\n",
      "Moving test folder...\n",
      "Moving test folder completed.\n",
      "Removed folder: down-camera-sim-8.\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"{}-{}\".format(roboflow_project_name, latest_version)\n",
    "roboflow_folder_path = os.path.abspath(folder_name)\n",
    "data_folder_path = os.path.abspath(\"data\")\n",
    "\n",
    "print(f\"Roboflow folder name: {folder_name}.\")\n",
    "\n",
    "def copy_all_files(source, destination):\n",
    "     try:\n",
    "          # Ensure destination folder exists.\n",
    "          os.makedirs(destination, exist_ok=True)\n",
    "          # Copy all files from source to destination.\n",
    "          for filename in os.listdir(source):\n",
    "               source_file = os.path.join(source, filename)\n",
    "               if os.path.isfile(source_file):\n",
    "                    destination_file = os.path.join(destination, filename)\n",
    "                    shutil.move(source_file, destination_file)\n",
    "     except Exception as e:\n",
    "          print(f\"Error copying files: {e}.\")\n",
    "\n",
    "# Copy files from roboflow to data folder.\n",
    "for folder in [\"train\", \"valid\", \"test\"]:\n",
    "     print(f\"Moving {folder} folder...\")\n",
    "     copy_all_files(os.path.join(roboflow_folder_path, folder, \"images\"), \n",
    "                    os.path.join(data_folder_path, \"raw\", \"images\"))\n",
    "     copy_all_files(os.path.join(roboflow_folder_path, folder, \"labels\"), \n",
    "                    os.path.join(data_folder_path, \"raw\", \"labels\"))\n",
    "     print(f\"Moving {folder} folder completed.\")\n",
    "\n",
    "# Optional: Remove roboflow folder after copying files.\n",
    "try:\n",
    "     shutil.rmtree(folder_name)\n",
    "     print(f\"Removed folder: {folder_name}.\")\n",
    "except Exception as e:\n",
    "     print(f\"Error removing folder: {e}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data, split into train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m raw_image_names, raw_label_names = get_file_names(in_folder)\n\u001b[32m      8\u001b[39m num_raw_samples = \u001b[38;5;28mlen\u001b[39m(raw_image_names)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mdo_augs_and_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_image_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_label_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m split_and_move_data(out_folder, out_folder)\n\u001b[32m     12\u001b[39m os.rmdir(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33maugmented\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mdo_augs_and_export\u001b[39m\u001b[34m(img_filenames, label_filenames, source_folder, output_folder)\u001b[39m\n\u001b[32m     60\u001b[39m name_num = \u001b[32m1\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (img_filename, label_filename) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(img_filenames, label_filenames):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     augs = \u001b[43mget_augs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(source_folder + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mlabels\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m + label_filename) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m#build array of bounding boxes (each line its own element)\u001b[39;00m\n\u001b[32m     65\u001b[39m         bounding_boxes = f.read()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mget_augs\u001b[39m\u001b[34m(img_filename, source_folder)\u001b[39m\n\u001b[32m     46\u001b[39m     augs = augs + colorAugment(augs)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(np.random.rand() < noiseAugmentProb):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     augs = augs + \u001b[43mnoiseAugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(np.random.rand() < resolutionAugmentProb):\n\u001b[32m     50\u001b[39m     augs = augs + resolutionAugment(augs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mnoiseAugment\u001b[39m\u001b[34m(images)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[32m     35\u001b[39m     transform = A.Compose([A.ISONoise(color_shift=(\u001b[32m0.1\u001b[39m, \u001b[32m0.1\u001b[39m), intensity=(\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m), always_apply=\u001b[38;5;28;01mTrue\u001b[39;00m)])\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     noisy_img = \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     37\u001b[39m     out.append(noisy_img)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/albumentations/core/composition.py:610\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, force_apply, *args, **data)\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;28mself\u001b[39m.preprocess(data)\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     data = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    611\u001b[39m     \u001b[38;5;28mself\u001b[39m._track_transform_params(t, data)\n\u001b[32m    612\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.check_data_post_transform(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/albumentations/core/transforms_interface.py:273\u001b[39m, in \u001b[36mBasicTransform.__call__\u001b[39m\u001b[34m(self, force_apply, *args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.deterministic:\n\u001b[32m    272\u001b[39m         kwargs[\u001b[38;5;28mself\u001b[39m.save_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] = deepcopy(params)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/albumentations/core/transforms_interface.py:310\u001b[39m, in \u001b[36mBasicTransform.apply_with_params\u001b[39m\u001b[34m(self, params, *args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    308\u001b[39m         target_function = \u001b[38;5;28mself\u001b[39m._key2func[key]\n\u001b[32m    309\u001b[39m         res[key] = ensure_contiguous_output(\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m             \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_contiguous_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    311\u001b[39m         )\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    313\u001b[39m     res[key] = arg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/albumentations/augmentations/pixel/transforms.py:2740\u001b[39m, in \u001b[36mISONoise.apply\u001b[39m\u001b[34m(self, img, color_shift, intensity, random_seed, **params)\u001b[39m\n\u001b[32m   2726\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Apply the ISONoise transform to the input image.\u001b[39;00m\n\u001b[32m   2727\u001b[39m \n\u001b[32m   2728\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2737\u001b[39m \n\u001b[32m   2738\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2739\u001b[39m non_rgb_error(img)\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfpixel\u001b[49m\u001b[43m.\u001b[49m\u001b[43miso_noise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolor_shift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2743\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2744\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2745\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/albucore/functions.py:815\u001b[39m, in \u001b[36mfloat32_io.<locals>.float32_wrapper\u001b[39m\u001b[34m(img, *args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_dtype != np.float32:\n\u001b[32m    814\u001b[39m     img = to_float(img)\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m from_float(result, target_dtype=input_dtype) \u001b[38;5;28;01mif\u001b[39;00m input_dtype != np.float32 \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/albucore/utils.py:120\u001b[39m, in \u001b[36mclipped.<locals>.wrapped_function\u001b[39m\u001b[34m(img, *args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.dtype == np.uint8:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/albucore/utils.py:108\u001b[39m, in \u001b[36mclip\u001b[39m\u001b[34m(img, dtype, inplace)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.clip(img, \u001b[32m0\u001b[39m, max_value, out=img)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_value\u001b[49m\u001b[43m)\u001b[49m.astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2330\u001b[39m, in \u001b[36mclip\u001b[39m\u001b[34m(a, a_min, a_max, out, min, max, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mmin\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mmax\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue:\n\u001b[32m   2327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing `min` or `max` keyword argument when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/numpy/_core/_methods.py:116\u001b[39m, in \u001b[36m_clip\u001b[39m\u001b[34m(a, min, max, out, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m um.maximum(a, \u001b[38;5;28mmin\u001b[39m, out=out, **kwargs)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mum\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "out_folder = f\"data{os_path}augmented\"\n",
    "in_folder = f\"data{os_path}raw\"\n",
    "train = f\"{os_path}train\"\n",
    "val = f\"{os_path}val\"\n",
    "test = f\"{os_path}test\"\n",
    "\n",
    "raw_image_names, raw_label_names = get_file_names(in_folder)\n",
    "num_raw_samples = len(raw_image_names)\n",
    "do_augs_and_export(raw_image_names, raw_label_names, in_folder, out_folder)\n",
    "split_and_move_data(out_folder, out_folder)\n",
    "\n",
    "os.rmdir(f\"data{os_path}augmented{os_path}images\")\n",
    "os.rmdir(f\"data{os_path}augmented{os_path}labels\")\n",
    "shutil.rmtree(in_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation completed: went from 414 samples to 5525 after augmentation.\n"
     ]
    }
   ],
   "source": [
    "augmented_train_img_names, _ = get_file_names(out_folder + train)\n",
    "augmented_test_img_names, _ = get_file_names(out_folder + test)\n",
    "augmented_val_img_names, _ = get_file_names(out_folder + val)\n",
    "\n",
    "num_augmented_samples = len(augmented_train_img_names) + len(augmented_test_img_names) + len(augmented_val_img_names)\n",
    "\n",
    "print(f\"Augmentation completed: went from {num_raw_samples} samples to {num_augmented_samples} after augmentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check CUDA dependencies and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA is NOT available. Please ensure that your system has a compatible GPU and the necessary drivers are installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (torch.cuda.is_available() \u001b[38;5;129;01mand\u001b[39;00m torch.cuda.device_count()):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCUDA is NOT available. Please ensure that your system has a compatible GPU and the necessary drivers are installed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA is NOT available. Please ensure that your system has a compatible GPU and the necessary drivers are installed."
     ]
    }
   ],
   "source": [
    "if not (torch.cuda.is_available() and torch.cuda.device_count()):\n",
    "    raise RuntimeError(\"CUDA is NOT available. Please ensure that your system has a compatible GPU and the necessary drivers are installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_folder = \"runs\"\n",
    "detect_folder = \"detect\"\n",
    "\n",
    "if os.path.exists(runs_folder):\n",
    "    shutil.rmtree(runs_folder)\n",
    "\n",
    "os.makedirs(os.path.join(runs_folder, detect_folder), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback method tracks losses on the training and validation data after each fit epoch (train + val) to test model convergence and fit\n",
    "\n",
    "# bounding box loss: This loss component measures how accurately the model predicts the spatial location and size of bounding boxes around detected objects.\n",
    "# classification loss: This loss component evaluates the model's ability to correctly classify the objects within the predicted bounding boxes.\n",
    "# objectness (confidence) loss: This loss component assesses the model's confidence that a predicted bounding box actually contains an object, as opposed to being background.\n",
    "\n",
    "train_box_losses = []\n",
    "train_cls_losses = []\n",
    "train_obj_losses = []\n",
    "val_box_losses = []\n",
    "val_cls_losses = []\n",
    "val_obj_losses = []\n",
    "\n",
    "def on_fit_epoch_end(trainer):\n",
    "    # get the results.csv data\n",
    "    results = pd.read_csv(trainer.csv)\n",
    "    # get the current epoch number from the trainer\n",
    "    current_epoch = trainer.epoch\n",
    "    # save epoch's losses\n",
    "    train_box_losses.append(results['           train/box_loss'].iloc[current_epoch])\n",
    "    train_cls_losses.append(results['           train/cls_loss'].iloc[current_epoch])\n",
    "    train_obj_losses.append(results['           train/obj_loss'].iloc[current_epoch])\n",
    "    val_box_losses.append(results['           val/box_loss'].iloc[current_epoch])\n",
    "    val_cls_losses.append(results['           val/cls_loss'].iloc[current_epoch])\n",
    "    val_obj_losses.append(results['           val/obj_loss'].iloc[current_epoch])\n",
    "\n",
    "    print(f\"End of epoch {current_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'crop_fraction' is deprecated and will be removed in the future.\n",
      "Ultralytics 8.3.202 🚀 Python-3.12.11 torch-2.8.0 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/Users/owenlesann/robotics/AUV-2025/catkin_ws/src/vision/model_pipeline/data.yaml, degrees=180, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=200, erasing=0.0, exist_ok=False, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.3, hsv_v=0.3, imgsz=[480, 640], int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.1, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/owenlesann/robotics/AUV-2025/catkin_ws/src/vision/model_pipeline/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.0, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.0, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING ⚠️ updating to 'imgsz=640'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 387.7±241.3 MB/s, size: 122.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/owenlesann/robotics/AUV-2025/catkin_ws/src/vision/model_pipeline/data/augmented/train/labels.cache... 3867 images, 1354 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 3867/3867 8.0Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640 at 60.0% CUDA memory utilization.\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mAutoBatch: \u001b[0mintended for CUDA devices, using default batch-size 16\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 297.7±100.5 MB/s, size: 122.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/owenlesann/robotics/AUV-2025/catkin_ws/src/vision/model_pipeline/data/augmented/train/labels.cache... 3867 images, 1354 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 3867/3867 7.4Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 605.6±193.4 MB/s, size: 232.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/owenlesann/robotics/AUV-2025/catkin_ws/src/vision/model_pipeline/data/augmented/val/labels.cache... 1105 images, 411 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1105/1105 1.3Mit/s 0.0s0s\n",
      "Plotting labels to /Users/owenlesann/robotics/AUV-2025/catkin_ws/src/vision/model_pipeline/runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/owenlesann/robotics/AUV-2025/catkin_ws/src/vision/model_pipeline/runs/detect/train\u001b[0m\n",
      "Starting training for 200 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/200         0G      2.357      6.459      1.673         15        640: 1% ──────────── 2/242 0.1it/s 26.8s<1:03:46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_yaml_file_absolute_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch_increments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdetect\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhsv_h\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhsv_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhsv_s\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhsv_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhsv_v\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhsv_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfliplr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfliplr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflipud\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflipud\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmosaic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmosaic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy_paste\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy_paste\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43merasing\u001b[49m\u001b[43m=\u001b[49m\u001b[43merasing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcrop_fraction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrop_fraction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdegrees\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m         shutil.copyfile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mruns\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mdetect\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mtrain\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mweights\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mbest.pt\u001b[39m\u001b[33m\"\u001b[39m, model_save_filename)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/model.py:800\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    797\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    798\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/trainer.py:235\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    232\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/trainer.py:429\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28mself\u001b[39m.tloss = (\n\u001b[32m    425\u001b[39m         (\u001b[38;5;28mself\u001b[39m.tloss * i + \u001b[38;5;28mself\u001b[39m.loss_items) / (i + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss_items\n\u001b[32m    426\u001b[39m     )\n\u001b[32m    428\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data_yaml_file_absolute_path = os.path.abspath(\"data.yaml\")\n",
    "\n",
    "model = YOLO(model_name) # load a pretrained model.\n",
    "\n",
    "# register fit callback\n",
    "model.add_callback(\"on_fit_epoch_end\", on_fit_epoch_end)\n",
    "\n",
    "# Start the training process.\n",
    "while True:\n",
    "    try:\n",
    "        model.train(\n",
    "            data=data_yaml_file_absolute_path,\n",
    "            epochs=epoch_increments,\n",
    "            imgsz=imgsz,\n",
    "            batch=batch_size,\n",
    "            pretrained=pretrained,\n",
    "            task=\"detect\",\n",
    "            cache=cache,\n",
    "            workers=workers,\n",
    "            hsv_h=hsv_h,\n",
    "            hsv_s=hsv_s,\n",
    "            hsv_v=hsv_v,\n",
    "            translate=translate,\n",
    "            scale=scale,\n",
    "            fliplr=fliplr,\n",
    "            flipud=flipud,\n",
    "            mosaic=mosaic,\n",
    "            copy_paste=copy_paste,\n",
    "            erasing=erasing,\n",
    "            crop_fraction=crop_fraction,\n",
    "            degrees=degrees\n",
    "        )\n",
    "        shutil.copyfile(f\"runs{os_path}detect{os_path}train{os_path}weights{os_path}best.pt\", model_save_filename)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Caught a RuntimeError: {e}.\")\n",
    "        break  # Break out of the loop if an error occurs to prevent infinite loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # 1 row, 3 columns\n",
    "\n",
    "# Box losses\n",
    "axes[0].plot(train_box_losses, label=\"train losses\")\n",
    "axes[0].plot(val_box_losses, label=\"validation losses\")\n",
    "axes[0].set_title(\"Box Losses Over Time\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Classification losses\n",
    "axes[1].plot(train_cls_losses, label=\"train losses\")\n",
    "axes[1].plot(val_cls_losses, label=\"validation losses\")\n",
    "axes[1].set_title(\"Classification Losses Over Time\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Objectness losses\n",
    "axes[2].plot(train_obj_losses, label=\"train losses\")\n",
    "axes[2].plot(val_obj_losses, label=\"validation losses\")\n",
    "axes[2].set_title(\"Objectness (Confidence) Losses Over Time\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.202 🚀 Python-3.12.11 torch-2.8.0 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 3,006,233 parameters, 13,065 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 474.5±179.2 MB/s, size: 170.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/owenlesann/robotics/AUV-2025/catkin_ws/src/vision/model_pipeline/data/augmented/val/labels.cache... 1105 images, 411 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1105/1105 2.6Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 6% ╸─────────── 4/70 0.2it/s 11.7s<5:59\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m metrics = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_yaml_file_absolute_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Save metric plots of model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/model.py:635\u001b[39m, in \u001b[36mModel.val\u001b[39m\u001b[34m(self, validator, **kwargs)\u001b[39m\n\u001b[32m    632\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m    634\u001b[39m validator = (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mvalidator\u001b[39m\u001b[33m\"\u001b[39m))(args=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;28mself\u001b[39m.metrics = validator.metrics\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m validator.metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/validator.py:214\u001b[39m, in \u001b[36mBaseValidator.__call__\u001b[39m\u001b[34m(self, trainer, model)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[32m2\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:637\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/tasks.py:139\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/tasks.py:157\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/tasks.py:180\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    181\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/modules/conv.py:93\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     86\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "metrics = model.val(data=data_yaml_file_absolute_path, plots=True) # Save metric plots of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
