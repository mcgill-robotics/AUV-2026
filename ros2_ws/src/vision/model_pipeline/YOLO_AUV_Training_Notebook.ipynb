{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edX5GkTFMdS0"
   },
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1758598142375,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "Z1PBpdg0MdS4"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "is_front_camera_training = False  # Change it to False if training for down cmaera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758598142385,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "89yJH2cyMdS5"
   },
   "outputs": [],
   "source": [
    "################################# OS PARAMETERS ##################################\n",
    "os_name = os.name\n",
    "# Windows = nt, [Linux, Apple] = posix.\n",
    "os_path = \"\\\\\" if os_name == \"nt\" else \"/\"\n",
    "##################################################################################\n",
    "############################## ROBOFLOW  PARAMETERS ##############################\n",
    "roboflow_api_key = \"u7zfMxAWzFvZlg4S2QTH\"\n",
    "roboflow_workspace_name = \"robosub-2025-mu9e7\"\n",
    "dataset_export_format = \"yolov8\"\n",
    "# Currently data is not labeled separately\n",
    "roboflow_project_name = \"douglas-vision-model\"\n",
    "# if is_front_camera_training:\n",
    "#      roboflow_project_name = \"down-camera-comp\"\n",
    "# else:\n",
    "#      roboflow_project_name = \"down-camera-sim\"\n",
    "##################################################################################\n",
    "############################## TRAINING  PARAMETERS ##############################\n",
    "# Likewise\n",
    "# if is_front_camera_training:\n",
    "#      target_classes = [\"bouy\", \"gate\", \"octagon-table\"]\n",
    "#      model_save_filename = \"best_AUV_sim_front_camera_model.pt\"\n",
    "# else:\n",
    "#      target_classes = [\"bin\", \"lane-marker\", \"octagon-table\"]\n",
    "#      model_save_filename = \"best_AUV_sim_down_camera_model.pt\"\n",
    "# Since camera models aren't split, we train on 6 classes\n",
    "# 2025 Number of class annotations (before augmentations) for those interested (delete later): \n",
    "# {\"gate\": 505, \"octagon_table\": 575, \"octagon_top\": 502, \"path_marker\": 693, \"sawfish\": 171, \"shark\": 168}\n",
    "target_classes = [\"gate\", \"octagon_table\", \"octagon_top\", \"path_marker\", \"sawfish\", \"shark\"]\n",
    "model_save_filename = \"best_AUV_sim_model.pt\"\n",
    "model_name = \"yolov8n.pt\"\n",
    "train_test_val_split = (0.7, 0.2, 0.1)\n",
    "epoch_increments = 200\n",
    "# This is likely what cause the Jetson Nano to crash\n",
    "# batch_size = -1 # Auto Mode (60% GPU Memory): Use batch=-1 to automatically\n",
    "#                 # adjust batch size for approximately 60% CUDA memory\n",
    "#                 # utilization.\n",
    "# TODO: Test setting batch size to 1 and then increment until jetson nano throws out-of-memory error. -1 scheduler works for laptops.\n",
    "batch_size = 1\n",
    "# TODO: Consider plotting speedup against number of processors/threads to test how many data loading threads are optimal for nano\n",
    "workers = 2\n",
    "cache = False\n",
    "pretrained = True\n",
    "hsv_h = 0.015\n",
    "hsv_s = 0.3\n",
    "hsv_v = 0.3\n",
    "translate = 0.0\n",
    "scale = 0.0\n",
    "fliplr = 0.5\n",
    "flipud = 0.5\n",
    "mosaic = 0.1\n",
    "copy_paste = 0.0\n",
    "erasing = 0.0\n",
    "fraction = 0.1\n",
    "degrees = 180\n",
    "##################################################################################\n",
    "######################### CUSTOM AUGMENTATION PARAMETERS #########################\n",
    "colorAugmentProb = 0.5\n",
    "noiseAugmentProb = 0.5\n",
    "resolutionAugmentProb = 0.0\n",
    "contrastAugmentProb = 0.5\n",
    "blurAugmentProb = 0.5\n",
    "brightnessAugmentProb = 0.5\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASKXCqQmMdS6"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYnA3UlpMdS6"
   },
   "source": [
    "## Setup python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26776,
     "status": "ok",
     "timestamp": 1758598169162,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "WvqrCspBMdS6",
    "outputId": "22faed83-4e47-4e84-fe72-8685dbabe09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (1.2.9)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2025.8.3)\n",
      "Requirement already satisfied: idna==3.7 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.4.9)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (3.10.6)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2.2.6)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (11.3.0)\n",
      "Requirement already satisfied: pi-heif<2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.1.0)\n",
      "Requirement already satisfied: pillow-avif-plugin<2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.1.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2.32.5)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (2.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib->roboflow) (1.3.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib->roboflow) (4.60.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib->roboflow) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib->roboflow) (3.2.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests->roboflow) (3.3.2)\n",
      "Requirement already satisfied: albumentations in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (1.16.2)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (2.11.9)\n",
      "Requirement already satisfied: albucore==0.0.24 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albucore==0.0.24->albumentations) (4.0.13)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from opencv-python) (2.2.6)\n",
      "Requirement already satisfied: ultralytics in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (8.3.202)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (3.10.6)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (1.16.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (2.8.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (0.23.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: polars in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (1.33.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from ultralytics) (2.0.17)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/yolo/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Packages installed successfully.\n"
     ]
    }
   ],
   "source": [
    "packages = [\n",
    "     \"roboflow\",\n",
    "     \"albumentations\",\n",
    "     \"opencv-python\",\n",
    "     \"ultralytics\",\n",
    "     \"pandas\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    print(\"Packages installed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"An error occurred: {e}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10408,
     "status": "ok",
     "timestamp": 1758598179575,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "PrqUeH7gMdS7",
    "outputId": "5dc4c6e4-52d8-4550-c6d8-91202db49edf"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from roboflow import Roboflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1811,
     "status": "ok",
     "timestamp": 1758598181392,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "vjuS08lRMdS7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "mkdir: data/augmented: File exists\n",
      "mkdir: data/augmented/images: File exists\n",
      "mkdir: data/augmented/labels: File exists\n",
      "mkdir: data/augmented/train: File exists\n",
      "mkdir: data/augmented/test: File exists\n",
      "mkdir: data/augmented/val: File exists\n",
      "mkdir: data/augmented/train/images: File exists\n",
      "mkdir: data/augmented/test/images: File exists\n",
      "mkdir: data/augmented/val/images: File exists\n",
      "mkdir: data/augmented/train/labels: File exists\n",
      "mkdir: data/augmented/test/labels: File exists\n",
      "mkdir: data/augmented/val/labels: File exists\n",
      "mkdir: data/raw: File exists\n",
      "mkdir: data/raw/images: File exists\n",
      "mkdir: data/raw/labels: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!mkdir data/augmented\n",
    "!mkdir data/augmented/images\n",
    "!mkdir data/augmented/labels\n",
    "!mkdir data/augmented/train\n",
    "!mkdir data/augmented/test\n",
    "!mkdir data/augmented/val\n",
    "!mkdir data/augmented/train/images\n",
    "!mkdir data/augmented/test/images\n",
    "!mkdir data/augmented/val/images\n",
    "!mkdir data/augmented/train/labels\n",
    "!mkdir data/augmented/test/labels\n",
    "!mkdir data/augmented/val/labels\n",
    "!mkdir data/raw\n",
    "!mkdir data/raw/images\n",
    "!mkdir data/raw/labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjPTIx8ZMdS7"
   },
   "source": [
    "## Define YOLO classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1758598181423,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "cEYERzI4MdS7",
    "outputId": "19736497-8724-40bd-c4db-03fe028e8775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data absolute path: /Users/owenlesann/robotics/AUV-2026/ros2_ws/src/vision/model_pipeline/data.\n"
     ]
    }
   ],
   "source": [
    "data_folder_absolute_path = os.path.abspath(\"data\")\n",
    "print(f\"Data absolute path: {data_folder_absolute_path}.\")\n",
    "\n",
    "with open(\"data.yaml\", \"w+\") as f:\n",
    "    f.write(f\"train: {data_folder_absolute_path}{os_path}augmented{os_path}train{os_path}images\\n\")\n",
    "    f.write(f\"test: {data_folder_absolute_path}{os_path}augmented{os_path}test{os_path}images\\n\")\n",
    "    f.write(f\"val: {data_folder_absolute_path}{os_path}augmented{os_path}val{os_path}images\\n\")\n",
    "    f.write(f\"nc: {len(target_classes)}\\n\")\n",
    "    f.write(f\"names: {target_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPIjYbG8MdS8"
   },
   "source": [
    "## Define augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1758598181436,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "zMt18ugyMdS8"
   },
   "outputs": [],
   "source": [
    "# Given a list of samples, make two copies of each sample that are darker/brighter to simulate differently lit environments.\n",
    "def brightnessAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([A.ColorJitter(brightness=(1.5, 1.5), contrast=0, saturation=0, hue=0, always_apply=True)])\n",
    "        bright_img = transform(image=image)[\"image\"]\n",
    "        transform = A.Compose([A.ColorJitter(brightness=(0.5, 0.5), contrast=0, saturation=0, hue=0, always_apply=True)])\n",
    "        dark_img = transform(image=image)[\"image\"]\n",
    "        out.append(bright_img)\n",
    "        out.append(dark_img)\n",
    "    return out\n",
    "\n",
    "# Given a list of samples, make a copy of each sample but more blurred to simulate objects out of focus, dirty lenses, and backscattering.\n",
    "def blurAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        ksize = (8, 8) # lower to lower blur\n",
    "        blurred_img = cv2.blur(image, ksize)\n",
    "        out.append(blurred_img)\n",
    "    return out\n",
    "\n",
    "# Given a list of samples, make a copy of each sample but with a lower contrast image to simulate backscattering and over/under-exposure.\n",
    "def contrastAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([A.ColorJitter (brightness=0, contrast=(0.5, 0.5), saturation=0, hue=0, always_apply=True)])\n",
    "        decontrasted_img = transform(image=image)[\"image\"]\n",
    "        out.append(decontrasted_img)\n",
    "    return out\n",
    "\n",
    "# Given a list of samples, make a copy of each sample but with camera noise added to the image to simulate different camera feeds.\n",
    "def noiseAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([A.ISONoise(color_shift=(0.1, 0.1), intensity=(0.5, 0.5), always_apply=True)])\n",
    "        noisy_img = transform(image=image)[\"image\"]\n",
    "        out.append(noisy_img)\n",
    "    return out\n",
    "\n",
    "# Given a list of samples, make a copy of each sample but with the image downscaled (lower resolution of image) to simulate lower quality cameras/images.\n",
    "def resolutionAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([A.Downscale(scale_min=0.2, scale_max=0.2, always_apply=True)])\n",
    "        low_res_img = transform(image=image)[\"image\"]\n",
    "        out.append(low_res_img)\n",
    "    return out\n",
    "\n",
    "# Increase intensity of blues in given image.\n",
    "def make_bluer(img, color_shift_intensity):\n",
    "    img_b, img_g, img_r = cv2.split(img) # Split by channel.\n",
    "    img_b = np.uint16(img_b)\n",
    "    img_b += color_shift_intensity\n",
    "    np.clip(img_b, 0, 255, out=img_b)\n",
    "    img_b = np.uint8(img_b)\n",
    "    img = cv2.merge((img_b, img_g, img_r)) # Merge adjusted channels.\n",
    "    del img_b\n",
    "    del img_g\n",
    "    del img_r\n",
    "    return img\n",
    "\n",
    "# Increase intensity of greens in given image.\n",
    "def make_greener(img, color_shift_intensity):\n",
    "    img_b, img_g, img_r = cv2.split(img) # Split by channel.\n",
    "    img_g = np.uint16(img_g)\n",
    "    img_g += color_shift_intensity\n",
    "    np.clip(img_g, 0, 255, out=img_g)\n",
    "    img_g = np.uint8(img_g)\n",
    "    img = cv2.merge((img_b, img_g, img_r)) # Merge adjusted channels.\n",
    "    del img_b\n",
    "    del img_g\n",
    "    del img_r\n",
    "    return img\n",
    "\n",
    "# Given a list of samples, make two copies of each sample (one bluer, one greener) to simulate different pools + color attenuation.\n",
    "def colorAugment(images):\n",
    "    out = []\n",
    "    color_shift_intensity = int(255*0.05)\n",
    "    for image in images:\n",
    "        blue_img = make_bluer(image, color_shift_intensity)\n",
    "        green_img = make_greener(image, color_shift_intensity)\n",
    "        out.append(blue_img)\n",
    "        out.append(green_img)\n",
    "    return out\n",
    "\n",
    "# Given a single image and augmentation function, displays the image before and images after augmentation.\n",
    "def visualizeAugmentation(img, aug):\n",
    "    # Show original image.\n",
    "    cv2.imshow(\"og\", img)\n",
    "    cv2.waitKey(0)\n",
    "    # Show all augmented images.\n",
    "    for augmented in aug([(img, \"\")])[1:]:\n",
    "        cv2.imshow(\"augmented\", augmented[0])\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1758598181483,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "6PPILubGMdS8"
   },
   "outputs": [],
   "source": [
    "def get_file_names(source_folder):\n",
    "    label_filenames = []\n",
    "    img_filenames = [f for f in os.listdir(source_folder + f\"{os_path}images\") if os.path.isfile(os.path.join(source_folder + f\"{os_path}images\", f))]\n",
    "    for img_filename in img_filenames:\n",
    "        label_filenames.append(os.path.splitext(img_filename)[0] + \".txt\")\n",
    "\n",
    "    return np.array(img_filenames), np.array(label_filenames)\n",
    "\n",
    "def split_and_move_data(source_dir, dest_dir, split_ratio=(0.7, 0.2, 0.1)):\n",
    "    source_image_dir = f\"{source_dir}{os_path}images\"\n",
    "    source_label_dir = f\"{source_dir}{os_path}labels\"\n",
    "    # Get list of image and label files.\n",
    "    image_files = sorted(os.listdir(source_image_dir))\n",
    "    label_files = sorted(os.listdir(source_label_dir))\n",
    "\n",
    "    # Shuffle the indices.\n",
    "    indices = np.arange(len(image_files))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Calculate split indices.\n",
    "    total_count = len(image_files)\n",
    "    train_end = int(total_count * split_ratio[0])\n",
    "    val_end = train_end + int(total_count * split_ratio[1])\n",
    "\n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "\n",
    "    # Function to move files\n",
    "    def move_files(indices, split):\n",
    "        for i in indices:\n",
    "            shutil.move(os.path.join(source_image_dir, image_files[i]),\n",
    "                        os.path.join(dest_dir, split, 'images', image_files[i]))\n",
    "            shutil.move(os.path.join(source_label_dir, label_files[i]),\n",
    "                        os.path.join(dest_dir, split, 'labels', label_files[i]))\n",
    "\n",
    "    # Move files to corresponding folders\n",
    "    move_files(train_indices, 'train')\n",
    "    move_files(val_indices, 'val')\n",
    "    move_files(test_indices, 'test')\n",
    "\n",
    "def get_augs(img_filename, source_folder):\n",
    "    img = cv2.imread(source_folder + f\"{os_path}images{os_path}\" + img_filename)\n",
    "    augs = [img]\n",
    "    if(np.random.rand() < colorAugmentProb):\n",
    "        augs = augs + colorAugment(augs)\n",
    "    if(np.random.rand() < noiseAugmentProb):\n",
    "        augs = augs + noiseAugment(augs)\n",
    "    if(np.random.rand() < resolutionAugmentProb):\n",
    "        augs = augs + resolutionAugment(augs)\n",
    "    if(np.random.rand() < contrastAugmentProb):\n",
    "        augs = augs + contrastAugment(augs)\n",
    "    if(np.random.rand() < blurAugmentProb):\n",
    "        augs = augs + blurAugment(augs)\n",
    "    if(np.random.rand() < brightnessAugmentProb):\n",
    "        augs = augs + brightnessAugment(augs)\n",
    "    return augs\n",
    "\n",
    "def do_augs_and_export(img_filenames, label_filenames, source_folder, output_folder):\n",
    "    name_num = 1\n",
    "    for (img_filename, label_filename) in zip(img_filenames, label_filenames):\n",
    "        augs = get_augs(img_filename, source_folder)\n",
    "        with open(source_folder + f\"{os_path}labels{os_path}\" + label_filename) as f:\n",
    "            #build array of bounding boxes (each line its own element)\n",
    "            bounding_boxes = f.read()\n",
    "        for aug in augs:\n",
    "            cv2.imwrite(output_folder + f\"{os_path}images{os_path}img\" + str(name_num) + \".png\", aug)\n",
    "            with open(output_folder + f\"{os_path}labels{os_path}img\" + str(name_num) + \".txt\", \"w+\") as f:\n",
    "                f.write(bounding_boxes)\n",
    "            name_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lzGbLXwMdS9"
   },
   "source": [
    "## Download dataset from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3472,
     "status": "ok",
     "timestamp": 1758598184960,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "hwvnNBIzMdS9",
    "outputId": "7c50402d-6822-4017-f20e-caec1dcfa403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Douglas-Vision-Model-8 to yolov8:: 100%|‚ñà| 10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Douglas-Vision-Model-8 in yolov8:: 100%|‚ñà| 239\n"
     ]
    }
   ],
   "source": [
    "rf = Roboflow(api_key=roboflow_api_key)\n",
    "project = rf.workspace(roboflow_workspace_name).project(roboflow_project_name)\n",
    "latest_version = int(project.versions()[0].version)\n",
    "version = project.version(latest_version)\n",
    "dataset = version.download(dataset_export_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1758598185033,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "NdBHUz_1MdS9",
    "outputId": "67cf9209-1cad-4457-a711-400697ba58a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roboflow folder name: douglas-vision-model-8.\n",
      "Moving train folder...\n",
      "Moving train folder completed.\n",
      "Moving valid folder...\n",
      "Moving valid folder completed.\n",
      "Moving test folder...\n",
      "Moving test folder completed.\n",
      "Removed folder: douglas-vision-model-8.\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"{}-{}\".format(roboflow_project_name, latest_version)\n",
    "roboflow_folder_path = os.path.abspath(folder_name)\n",
    "data_folder_path = os.path.abspath(\"data\")\n",
    "\n",
    "print(f\"Roboflow folder name: {folder_name}.\")\n",
    "\n",
    "def copy_all_files(source, destination):\n",
    "     try:\n",
    "          # Ensure destination folder exists.\n",
    "          os.makedirs(destination, exist_ok=True)\n",
    "          # Copy all files from source to destination.\n",
    "          for filename in os.listdir(source):\n",
    "               source_file = os.path.join(source, filename)\n",
    "               if os.path.isfile(source_file):\n",
    "                    destination_file = os.path.join(destination, filename)\n",
    "                    shutil.move(source_file, destination_file)\n",
    "     except Exception as e:\n",
    "          print(f\"Error copying files: {e}.\")\n",
    "\n",
    "# Copy files from roboflow to data folder.\n",
    "for folder in [\"train\", \"valid\", \"test\"]:\n",
    "     print(f\"Moving {folder} folder...\")\n",
    "     copy_all_files(os.path.join(roboflow_folder_path, folder, \"images\"),\n",
    "                    os.path.join(data_folder_path, \"raw\", \"images\"))\n",
    "     copy_all_files(os.path.join(roboflow_folder_path, folder, \"labels\"),\n",
    "                    os.path.join(data_folder_path, \"raw\", \"labels\"))\n",
    "     print(f\"Moving {folder} folder completed.\")\n",
    "\n",
    "# Optional: Remove roboflow folder after copying files.\n",
    "try:\n",
    "     shutil.rmtree(folder_name)\n",
    "     print(f\"Removed folder: {folder_name}.\")\n",
    "except Exception as e:\n",
    "     print(f\"Error removing folder: {e}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmqJF8yGMdS-"
   },
   "source": [
    "## Augment data, split into train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 74098,
     "status": "ok",
     "timestamp": 1758598259160,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "5Mu7ywZpMdS-"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "out_folder = f\"data{os_path}augmented\"\n",
    "in_folder = f\"data{os_path}raw\"\n",
    "train = f\"{os_path}train\"\n",
    "val = f\"{os_path}val\"\n",
    "test = f\"{os_path}test\"\n",
    "\n",
    "raw_image_names, raw_label_names = get_file_names(in_folder)\n",
    "num_raw_samples = len(raw_image_names)\n",
    "do_augs_and_export(raw_image_names, raw_label_names, in_folder, out_folder)\n",
    "split_and_move_data(out_folder, out_folder)\n",
    "\n",
    "os.rmdir(f\"data{os_path}augmented{os_path}images\")\n",
    "os.rmdir(f\"data{os_path}augmented{os_path}labels\")\n",
    "shutil.rmtree(in_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1758598259188,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "cXCkrrEWMdS-",
    "outputId": "638bea4f-ab0d-41d2-e344-87f36a1415d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation completed: went from 1191 samples to 16516 after augmentation.\n"
     ]
    }
   ],
   "source": [
    "augmented_train_img_names, _ = get_file_names(out_folder + train)\n",
    "augmented_test_img_names, _ = get_file_names(out_folder + test)\n",
    "augmented_val_img_names, _ = get_file_names(out_folder + val)\n",
    "\n",
    "num_augmented_samples = len(augmented_train_img_names) + len(augmented_test_img_names) + len(augmented_val_img_names)\n",
    "\n",
    "print(f\"Augmentation completed: went from {num_raw_samples} samples to {num_augmented_samples} after augmentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ciR_xKCMdS-"
   },
   "source": [
    "# Check CUDA dependencies and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758598259237,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "Be5VCLOGMdS-"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA is NOT available. Please ensure that your system has a compatible GPU and the necessary drivers are installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (torch.cuda.is_available() \u001b[38;5;129;01mand\u001b[39;00m torch.cuda.device_count()):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCUDA is NOT available. Please ensure that your system has a compatible GPU and the necessary drivers are installed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA is NOT available. Please ensure that your system has a compatible GPU and the necessary drivers are installed."
     ]
    }
   ],
   "source": [
    "if not (torch.cuda.is_available() and torch.cuda.device_count()):\n",
    "    raise RuntimeError(\"CUDA is NOT available. Please ensure that your system has a compatible GPU and the necessary drivers are installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758598259299,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "EDPEbr99MdS_"
   },
   "outputs": [],
   "source": [
    "runs_folder = \"runs\"\n",
    "detect_folder = \"detect\"\n",
    "\n",
    "if os.path.exists(runs_folder):\n",
    "    shutil.rmtree(runs_folder)\n",
    "\n",
    "os.makedirs(os.path.join(runs_folder, detect_folder), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1758598259347,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "C2BsY2GuMdS_"
   },
   "outputs": [],
   "source": [
    "# callback method tracks losses on the training and validation data after each fit epoch (train + val) to test model convergence and fit\n",
    "\n",
    "# bounding box loss: This loss component measures how accurately the model predicts the spatial location and size of bounding boxes around detected objects.\n",
    "# classification loss: This loss component evaluates the model's ability to correctly classify the objects within the predicted bounding boxes.\n",
    "# distribution focal loss: This loss component assesses the model's robustness to class imbalances\n",
    "train_box_losses, train_cls_losses, train_dfl_losses = [], [], []\n",
    "val_box_losses, val_cls_losses, val_dfl_losses = [], [], []\n",
    "\n",
    "def on_fit_epoch_end(trainer):\n",
    "    # get the results.csv data\n",
    "    results = pd.read_csv(trainer.csv)\n",
    "    # get the current epoch number from the trainer\n",
    "    current_epoch = trainer.epoch\n",
    "    # save epoch's losses\n",
    "    last_row = results.iloc[-1] # Gets results of latest epoch\n",
    "    train_box_losses.append(last_row[\"train/box_loss\"])\n",
    "    train_cls_losses.append(last_row[\"train/cls_loss\"])\n",
    "    train_dfl_losses.append(last_row[\"train/dfl_loss\"])\n",
    "    val_box_losses.append(last_row[\"val/box_loss\"])\n",
    "    val_cls_losses.append(last_row[\"val/cls_loss\"])\n",
    "    val_dfl_losses.append(last_row[\"val/dfl_loss\"])\n",
    "\n",
    "    print(f\"End of epoch {current_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "error",
     "timestamp": 1758644157247,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "r3yM271IMdS_",
    "outputId": "1c18bf4d-caee-448d-ff50-12732c83f60c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.218 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.202 üöÄ Python-3.12.11 torch-2.8.0 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/Users/owenlesann/robotics/AUV-2026/ros2_ws/src/vision/model_pipeline/data.yaml, degrees=180, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=200, erasing=0.0, exist_ok=False, fliplr=0.5, flipud=0.5, format=torchscript, fraction=0.1, freeze=None, half=False, hsv_h=0.015, hsv_s=0.3, hsv_v=0.3, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.1, multi_scale=False, name=train10, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/owenlesann/robotics/AUV-2026/ros2_ws/src/vision/model_pipeline/runs/detect/train10, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.0, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.0, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752482  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,012,018 parameters, 3,012,002 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 389.7¬±197.9 MB/s, size: 724.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/owenlesann/robotics/AUV-2026/ros2_ws/src/vision/model_pipeline/data/augmented/train/labels... 1156 images, 116 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1156/1156 400.6it/s 2.9s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/owenlesann/robotics/AUV-2026/ros2_ws/src/vision/model_pipeline/data/augmented/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 721.0¬±130.3 MB/s, size: 637.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/owenlesann/robotics/AUV-2026/ros2_ws/src/vision/model_pipeline/data/augmented/val/labels.cache... 3303 images, 380 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3303/3303 7.4Mit/s 0.0s\n",
      "Plotting labels to /Users/owenlesann/robotics/AUV-2026/ros2_ws/src/vision/model_pipeline/runs/detect/train10/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/owenlesann/robotics/AUV-2026/ros2_ws/src/vision/model_pipeline/runs/detect/train10\u001b[0m\n",
      "Starting training for 200 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/200         0G      2.521      4.687      2.393         18        640: 13% ‚îÅ‚ï∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 147/1156 3.5it/s 41.5s<4:45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_yaml_file_absolute_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch_increments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdetect\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhsv_h\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhsv_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhsv_s\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhsv_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhsv_v\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhsv_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfliplr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfliplr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflipud\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflipud\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmosaic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmosaic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy_paste\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy_paste\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43merasing\u001b[49m\u001b[43m=\u001b[49m\u001b[43merasing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdegrees\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m         shutil.copyfile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mruns\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mdetect\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mtrain\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mweights\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mbest.pt\u001b[39m\u001b[33m\"\u001b[39m, model_save_filename)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/model.py:800\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    797\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    798\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/trainer.py:235\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    232\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/trainer.py:433\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     last_opt_step = ni\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/trainer.py:669\u001b[39m, in \u001b[36mBaseTrainer.optimizer_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.unscale_(\u001b[38;5;28mself\u001b[39m.optimizer)  \u001b[38;5;66;03m# unscale gradients\u001b[39;00m\n\u001b[32m    668\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), max_norm=\u001b[32m10.0\u001b[39m)  \u001b[38;5;66;03m# clip gradients\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m    671\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/amp/grad_scaler.py:388\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke ``unscale_(optimizer)`` followed by parameter update, if gradients are not infs/NaN.\u001b[39;00m\n\u001b[32m    367\u001b[39m \n\u001b[32m    368\u001b[39m \u001b[33;03m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m \u001b[33;03m    Closure use is not currently supported.\u001b[39;00m\n\u001b[32m    386\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enabled:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosure\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    393\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/optim/optimizer.py:149\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/optim/adam.py:949\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/optim/adam.py:464\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    462\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m.addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    467\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data_yaml_file_absolute_path = os.path.abspath(\"data.yaml\")\n",
    "\n",
    "model = YOLO(model_name) # load a pretrained model.\n",
    "\n",
    "# register fit callback\n",
    "model.add_callback(\"on_fit_epoch_end\", on_fit_epoch_end)\n",
    "\n",
    "# Start the training process.\n",
    "while True:\n",
    "    try:\n",
    "        model.train(\n",
    "            data=data_yaml_file_absolute_path,\n",
    "            epochs=epoch_increments,\n",
    "            batch=batch_size,\n",
    "            pretrained=pretrained,\n",
    "            task=\"detect\",\n",
    "            cache=cache,\n",
    "            workers=workers,\n",
    "            hsv_h=hsv_h,\n",
    "            hsv_s=hsv_s,\n",
    "            hsv_v=hsv_v,\n",
    "            translate=translate,\n",
    "            scale=scale,\n",
    "            fliplr=fliplr,\n",
    "            flipud=flipud,\n",
    "            mosaic=mosaic,\n",
    "            copy_paste=copy_paste,\n",
    "            erasing=erasing,\n",
    "            fraction=fraction,\n",
    "            degrees=degrees\n",
    "        )\n",
    "        shutil.copyfile(f\"runs{os_path}detect{os_path}train{os_path}weights{os_path}best.pt\", model_save_filename)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Caught a RuntimeError: {e}.\")\n",
    "        break  # Break out of the loop if an error occurs to prevent infinite loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "error",
     "timestamp": 1758644163384,
     "user": {
      "displayName": "Owen Le Sann",
      "userId": "14841495162809323800"
     },
     "user_tz": 240
    },
    "id": "GIxzu6ijMdS_",
    "outputId": "c56ac240-08ac-4cc0-92c9-83c62e6179f6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # 1 row, 3 columns\n",
    "\n",
    "# Box losses\n",
    "axes[0].plot(train_box_losses, label=\"train losses\")\n",
    "axes[0].plot(val_box_losses, label=\"validation losses\")\n",
    "axes[0].set_title(\"Box Losses Over Time\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Classification losses\n",
    "axes[1].plot(train_cls_losses, label=\"train losses\")\n",
    "axes[1].plot(val_cls_losses, label=\"validation losses\")\n",
    "axes[1].set_title(\"Classification Losses Over Time\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Distribution focal losses\n",
    "axes[2].plot(train_dfl_losses, label=\"train losses\")\n",
    "axes[2].plot(val_dfl_losses, label=\"validation losses\")\n",
    "axes[2].set_title(\"Distribution Focal Losses Over Time\")\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Loss\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"losses_over_time.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "bSJj5etLMdTA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.202 üöÄ Python-3.12.11 torch-2.8.0 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 3,006,818 parameters, 13,650 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 581.2¬±307.6 MB/s, size: 734.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/owenlesann/robotics/AUV-2026/ros2_ws/src/vision/model_pipeline/data/augmented/val/labels.cache... 3303 images, 380 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3303/3303 7.1Mit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 1% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 3/207 0.1it/s 11.6s<27:06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m metrics_val = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_yaml_file_absolute_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Save metric plots of model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/model.py:635\u001b[39m, in \u001b[36mModel.val\u001b[39m\u001b[34m(self, validator, **kwargs)\u001b[39m\n\u001b[32m    632\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m    634\u001b[39m validator = (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mvalidator\u001b[39m\u001b[33m\"\u001b[39m))(args=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;28mself\u001b[39m.metrics = validator.metrics\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m validator.metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/engine/validator.py:214\u001b[39m, in \u001b[36mBaseValidator.__call__\u001b[39m\u001b[34m(self, trainer, model)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[32m2\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:637\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/tasks.py:139\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/tasks.py:157\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/tasks.py:180\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    181\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/ultralytics/nn/modules/conv.py:93\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     86\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/yolo/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "metrics_val = model.val(data=data_yaml_file_absolute_path, plots=True) # Save metric plots of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONhnqAC3MdTA"
   },
   "outputs": [],
   "source": [
    "# Only run this test after finished tuning model\n",
    "metrics_test = model.val(data=data_yaml_file_absolute_path, split=\"test\", plots=True) # Save metric plots of model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:yolo]",
   "language": "python",
   "name": "conda-env-yolo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
